1  import pandas as pd
2  from sklearn.feature_extraction.text import CountVectorizer
3  from sklearn.feature_extraction.text import TfidfVectorizer
4  from textblob import TextBlob
5  from sklearn.cluster import KMeans
6 
7  # open and read from the txt file
8  path = "C:/Users/marius.cerbauskas/Desktop/article1.txt"
9  file = open(path, 'r')
10 sentences = file.readlines()
11
12
13 def textblob_tokenizer(str_input):
14    blob = TextBlob(str_input.lower())
15    tokens = blob.words
16    words = [token.stem() for token in tokens]
17    return words
18
19
20 tf_vec = TfidfVectorizer(tokenizer=textblob_tokenizer,
21                      stop_words='english',
22                      norm='l2', # l2 – euclidean distance which is by default
23                      use_idf=True, # by default its true
24                      )
25 tf_matrix = tf_vec.fit_transform(sentences)
26 tf_df = pd.DataFrame(tf_matrix.toarray(), columns=tf_vec.get_feature_names())
27
28 # with pd.option_context('display.max_rows', 13, 'display.max_columns', 800):
29 #     print(tf_df)
30
31 number_of_clusters = 6
32 km = KMeans(n_clusters = number_of_clusters)
33 # Normally people fit the matrix
34 km.fit(tf_matrix)
35 # But you could fit the idf_df instead
36
37 print("Top terms per cluster:")
38 order_centroids = km.cluster_centers_.argsort()[:, ::–1]
39 terms = tf_vec.get_feature_names()
40 for i in range(number_of_clusters):
41    top_ten_words = [terms[ind] for ind in order_centroids[i, :12]]
42    print("Cluster {}: {}".format(i, ' '.join(top_ten_words)))
